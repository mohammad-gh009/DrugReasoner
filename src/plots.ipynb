{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b91bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Okabe–Ito palette\n",
    "cb_palette = [\n",
    "    \"#E69F00\",  # Orange\n",
    "    \"#56B4E9\",  # Sky Blue\n",
    "    \"#009E73\",  # Bluish Green\n",
    "    \"#F0E442\",  # Yellow\n",
    "    \"#0072B2\",  # Blue\n",
    "    \"#D55E00\",  # Vermillion\n",
    "    \"#CC79A7\"   # Reddish Purple\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../outputs\"\n",
    "\n",
    "csv_files = glob.glob(os.path.join(directory, \"*_test.csv\")) + \\\n",
    "            glob.glob(os.path.join(directory, \"*_val.csv\")) + \\\n",
    "            glob.glob(os.path.join(directory, \"*_external.csv\"))\n",
    "dfs = {}\n",
    "\n",
    "for file_path in csv_files:\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    dfs[file_name] = pd.read_csv(file_path)\n",
    "\n",
    "dfs['reasoner_val'] = dfs['reasoner_val'].replace({\"unapproved\": 0 , \"approved\": 1})\n",
    "dfs['reasoner_test'] = dfs['reasoner_test'].replace({\"unapproved\": 0 , \"approved\": 1})\n",
    "dfs['reasoner_external'] = dfs['reasoner_external'].replace({\"unapproved\": 0 , \"approved\": 1})\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(dfs)} DataFrames: {list(dfs.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cab982",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_results = []\n",
    "val_results = []\n",
    "external_results = []\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    y_true = df[\"y_true\"]\n",
    "    y_pred = df[\"y_pred\"]\n",
    "\n",
    "    # Compute metrics\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auc = np.nan  \n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)  \n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    row = {\n",
    "        \"Model\": name,\n",
    "        \"AUC\": auc,\n",
    "        \"F1-score\": f1,\n",
    "        \"Recall (Sensitivity)\": recall,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Precision\": precision,\n",
    "    }\n",
    "\n",
    "    \n",
    "    if name.endswith(\"_test\"):\n",
    "        test_results.append(row)\n",
    "    elif name.endswith(\"_val\"):\n",
    "        val_results.append(row)\n",
    "    elif name.endswith(\"_external\"):\n",
    "        external_results.append(row)\n",
    "\n",
    "test_df = pd.DataFrame(test_results).round(3)\n",
    "test_df.to_csv(\"../outputs/tables/test_results.csv\")\n",
    "val_df = pd.DataFrame(val_results).round(3)\n",
    "val_df.to_csv(\"../outputs/tables/validation_results.csv\")\n",
    "external_df = pd.DataFrame(external_results).round(3)\n",
    "external_df.to_csv(\"../outputs/tables/external_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f38ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary for the desired order\n",
    "order_mapping = {\n",
    "    'svm_test': 2,\n",
    "    'knn_test': 1, \n",
    "    'xgboost_test': 3,\n",
    "    'logestic_test': 0,\n",
    "    'reasoner_test': 4\n",
    "}\n",
    "\n",
    "test_df['sort_order'] = test_df['Model'].map(order_mapping)\n",
    "test_df = test_df.sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "order_mapping = {\n",
    "    'svm_val': 2,\n",
    "    'knn_val': 1, \n",
    "    'xgboost_val': 3,\n",
    "    'logestic_val': 0,\n",
    "    'reasoner_val': 4\n",
    "}\n",
    "val_df['sort_order'] = val_df['Model'].map(order_mapping)\n",
    "val_df = val_df.sort_values('sort_order').drop('sort_order', axis=1)\n",
    "\n",
    "order_mapping = {\n",
    "    'svm_external': 2,\n",
    "    'knn_external': 1, \n",
    "    'xgboost_external': 3,\n",
    "    'logestic_external': 0,\n",
    "    'reasoner_external': 5 ,\n",
    "    'ChemAp_external' :4\n",
    "}\n",
    "external_df['sort_order'] = external_df['Model'].map(order_mapping)\n",
    "external_df = external_df.sort_values('sort_order').drop('sort_order', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d9fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "def plot_metrics(df, title):\n",
    "    # Melt DataFrame for seaborn\n",
    "    df_melted = df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "    # Optional: order metrics for consistent display\n",
    "    metric_order = [\"AUC\", \"F1-score\", \"Recall (Sensitivity)\", \"Specificity\", \"Precision\" ]\n",
    "    df_melted[\"Metric\"] = pd.Categorical(df_melted[\"Metric\"], categories=metric_order, ordered=True)\n",
    "\n",
    "    # Plot with metrics on x-axis and models as hue, using Okabe–Ito palette\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        data=df_melted,\n",
    "        x=\"Metric\",\n",
    "        y=\"Score\",\n",
    "        hue=\"Model\",\n",
    "        palette=cb_palette  # \n",
    "    )\n",
    "\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.ylabel(\"Score\", fontsize=12)\n",
    "    plt.xlabel(\"Metric\", fontsize=12)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"../outputs/figures/{title}.eps\")\n",
    "    plt.savefig(f\"../outputs/figures/{title}.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df[\"Model\"] = test_df[\"Model\"].str.replace(r\"(_test|_val)$\", \"\", regex=True)\n",
    "val_df[\"Model\"] = val_df[\"Model\"].str.replace(r\"(_test|_val)$\", \"\", regex=True)\n",
    "external_df[\"Model\"] = external_df[\"Model\"].str.replace(r\"(_external)$\", \"\", regex=True)\n",
    "# Plot for test and validation DataFrames\n",
    "plot_metrics(test_df, \"Test Set Metrics\")\n",
    "plot_metrics(val_df, \"Validation Set Metrics\")\n",
    "plot_metrics(external_df, \"External Set Metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f56414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(ax, y_true, y_pred, title, cmap):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, cbar=False,\n",
    "                xticklabels=['Pred 0', 'Pred 1'],\n",
    "                yticklabels=['True 0', 'True 1'],\n",
    "                ax=ax)\n",
    "    ax.set_title(title, fontsize=14)\n",
    "    ax.set_ylabel('True Label')\n",
    "    ax.set_xlabel('Predicted Label')\n",
    "\n",
    "# Separate test and validation datasets\n",
    "test_dfs = {name: df for name, df in dfs.items() if name.endswith(\"_test\")}\n",
    "val_dfs = {name: df for name, df in dfs.items() if name.endswith(\"_val\")}\n",
    "external_dfs = {name: df for name, df in dfs.items() if name.endswith(\"_external\")}\n",
    "\n",
    "\n",
    "# Function to plot and save confusion matrices\n",
    "def plot_and_save(dfs, dataset_type, cmap, output_file):\n",
    "    # Determine subplot grid size\n",
    "    n = len(dfs)\n",
    "    cols = 2  # number of columns in the grid\n",
    "    rows = (n + cols - 1) // cols  # ceiling division\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 5 * rows))\n",
    "    axes = axes.flatten()  # Flatten axes for easy indexing\n",
    "\n",
    "    for idx, (name, df) in enumerate(dfs.items()):\n",
    "        y_true = df[\"y_true\"]\n",
    "        y_pred = df[\"y_pred\"]\n",
    "        clean_name = name.replace(\"_test\", \"\").replace(\"_val\", \"\")\n",
    "        \n",
    "        plot_confusion_matrix(axes[idx], y_true, y_pred, \n",
    "                             f\"{clean_name}\", cmap=cmap)\n",
    "\n",
    "    # Hide any unused subplots\n",
    "    for ax in axes[len(dfs):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Add main title for the entire figure\n",
    "    plt.suptitle(f\"Confusion Matrices for {dataset_type} Set\", fontsize=16, y=1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_file}.eps\")\n",
    "    plt.savefig(f\"{output_file}.eps\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Plot and save for test set (using Greens)\n",
    "plot_and_save(test_dfs, \"Test\", \"Greens\", \"../outputs/figures/Test_ConfusionMatrices\")\n",
    "\n",
    "# Plot and save for validation set (using Blues)\n",
    "plot_and_save(val_dfs, \"Validation\", \"Blues\", \"../outputs/figures/Validation_ConfusionMatrices\")\n",
    "\n",
    "# Plot and save for external set (using Reds)\n",
    "plot_and_save(external_dfs, \"External\", \"Reds\", \"../outputs/figures/External_ConfusionMatrices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f49aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "\n",
    "    TN, FP, FN, TP = cm.ravel() if cm.shape == (2,2) else (0,0,0,0)\n",
    "\n",
    "\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_pred)\n",
    "    except ValueError:\n",
    "        auc = 0.0  \n",
    "\n",
    "    results = {\n",
    "        \"F1 Score\": f1,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"AUC\": auc,\n",
    "        \"Specificity\": specificity,\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "# concat all dataframes. \n",
    "dfs = []\n",
    "\n",
    "df_0 = pd.read_csv('../outputs/checkpoins_outputs/zeroshot_base.csv')\n",
    "df_0['checkpoint'] = 0\n",
    "dfs.append(df_0)\n",
    "\n",
    "for ckpt in range(500, 15000, 500):\n",
    "    filename = f'../outputs/checkpoins_outputs/prediction_df_1.0_0.9_9_{ckpt}_final_setting.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    df['checkpoint'] = ckpt\n",
    "    dfs.append(df)\n",
    "\n",
    "all_df = pd.concat(dfs, ignore_index=True)\n",
    "all_df.reset_index(drop=True, inplace=True)\n",
    "all_df_valid = all_df[all_df['y_pred'].isin(['approved', 'unapproved'])]\n",
    "\n",
    "valid_percent = (all_df_valid.groupby('checkpoint').size()/451*100).reset_index(name='valid_y_pred')\n",
    "\n",
    "all_df_valid['y_pred_bin'] = all_df_valid['y_pred'].map({'unapproved': 0, 'approved': 1})\n",
    "all_df_valid['y_true_bin'] = all_df_valid['y_true'].map({'unapproved': 0, 'approved': 1})\n",
    "results_by_checkpoint = []\n",
    "\n",
    "for checkpoint, group in all_df_valid.groupby('checkpoint'):\n",
    "    y_true = group['y_true_bin']\n",
    "    y_pred = group['y_pred_bin']\n",
    "    try:\n",
    "        metrics = evaluate(y_true, y_pred)\n",
    "        metrics['Checkpoint'] = checkpoint\n",
    "        metrics['n_samples'] = len(group)\n",
    "        results_by_checkpoint.append(metrics)\n",
    "    except ValueError as e:\n",
    "        print(f\"Skipped checkpoint {checkpoint} due to error: {e}\")\n",
    "results_df = pd.DataFrame(results_by_checkpoint).sort_values('Checkpoint')\n",
    "# Okabe–Ito palette\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../outputs/checkpoints_evaluations.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1540686",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "results_df['Normalized Samples'] = results_df['n_samples'] / 451 *100\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(40, 10))\n",
    "\n",
    "metrics_to_plot = [ 'F1 Score', 'Precision', 'Recall', 'AUC', 'Specificity']\n",
    "for idx, metric in enumerate(metrics_to_plot):\n",
    "    sns.lineplot(\n",
    "        data=results_df,\n",
    "        x='Checkpoint',\n",
    "        y=metric,\n",
    "        ax=ax1,\n",
    "        label=metric,\n",
    "        marker='o',\n",
    "        linewidth=1.5,\n",
    "        color=cb_palette[idx % len(cb_palette)]\n",
    "    )\n",
    "\n",
    "ax1.set_xticks(np.arange(0, 14501, 500))\n",
    "ax1.set_xlim([0, 14500])\n",
    "ax1.set_xlabel(\"Checkpoint\")\n",
    "ax1.set_ylabel(\"Evaluation Metrics\")\n",
    "ax1.set_title(\"Model Evaluation Metrics Across Checkpoints\")\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "sns.lineplot(\n",
    "    data=results_df,\n",
    "    x='Checkpoint',\n",
    "    y='Normalized Samples',\n",
    "    ax=ax2,\n",
    "    color='black',\n",
    "    linestyle='--',\n",
    "    marker='o',\n",
    "    label='Valid Samples (%)'\n",
    ")\n",
    "ax2.set_ylabel(\"Valid Samples (%)\")\n",
    "# Combine legends\n",
    "handles1, labels1 = ax1.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend_.remove()\n",
    "ax2.legend_.remove()\n",
    "\n",
    "ax1.legend(\n",
    "    handles1 + handles2,\n",
    "    labels1 + labels2,\n",
    "    loc='center left',\n",
    "    bbox_to_anchor=(1.05, 0.5),\n",
    "    title=\"Metrics & Samples\"\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/Checkpoints_Evaluations.eps\")\n",
    "plt.savefig(\"../outputs/figures/Checkpoints_Evaluations.png\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(\"trainer_state.json\", \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Assuming json_data is already loaded, extract log_history\n",
    "data = json_data[\"log_history\"]\n",
    "\n",
    "# Filter data to only include steps that are multiples of 50\n",
    "# filtered_data = [entry for entry in data if entry['step'] % 50 == 0]\n",
    "\n",
    "# Extract steps and reward metrics from filtered data\n",
    "steps = [entry['step'] for entry in data]\n",
    "reward = [entry['reward'] for entry in data]\n",
    "confident_score = [entry['rewards/confident_score_func'] for entry in data]\n",
    "correctness = [entry['rewards/correctness_rewards'] for entry in data]\n",
    "int_reward = [entry['rewards/int_reward_func'] for entry in data]\n",
    "soft_format = [entry['rewards/soft_format_reward_func'] for entry in data]\n",
    "xmlcount = [entry['rewards/xmlcount_reward_func'] for entry in data]\n",
    "\n",
    "# Define exponential moving average function\n",
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate exponential moving average\n",
    "    alpha: smoothing factor (0 < alpha <= 1)\n",
    "    Lower alpha = more smoothing\n",
    "    \"\"\"\n",
    "    ema = np.zeros_like(data)\n",
    "    ema[0] = data[0]  # Initialize with first value\n",
    "    for i in range(1, len(data)):\n",
    "        ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]\n",
    "    return ema\n",
    "\n",
    "# Smoothing factor for EMA (adjust as needed)\n",
    "alpha = 0.05  # Lower value = more smoothing\n",
    "\n",
    "# Colorblind-friendly palette\n",
    "cb_palette = [\n",
    "    \"#E69F00\",  # Orange\n",
    "    \"#56B4E9\",  # Sky Blue\n",
    "    \"#009E73\",  # Bluish Green\n",
    "    \"#F0E442\",  # Yellow\n",
    "    \"#0072B2\",  # Blue\n",
    "    \"#D55E00\",  # Vermillion\n",
    "    \"#CC79A7\"   # Reddish Purple\n",
    "]\n",
    "\n",
    "# List of reward metrics and their labels, using colors from cb_palette\n",
    "metrics = [\n",
    "    (reward, 'Total Reward', cb_palette[0]),\n",
    "    (confident_score, 'Confidence-alignment', cb_palette[1]),\n",
    "    (correctness, 'Correctness', cb_palette[2]),\n",
    "    (int_reward, 'Interpretability', cb_palette[3]),\n",
    "    (soft_format, 'Soft format compliance', cb_palette[4]),\n",
    "    (xmlcount, 'XML format', cb_palette[5])\n",
    "]\n",
    "\n",
    "# Create a single figure with subplots (one column, one subplot per metric)\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4 * len(metrics)), sharex=True)\n",
    "\n",
    "# Ensure axes is a list for iteration, even with one subplot\n",
    "if len(metrics) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot each metric in its own subplot\n",
    "for ax, (metric_data, label, color) in zip(axes, metrics):\n",
    "    # Plot original data faintly\n",
    "    ax.plot(steps, metric_data, color=color, alpha=0.3, label=f'Raw')\n",
    "    # Calculate and plot exponential moving average with thicker line\n",
    "    ema_data = exponential_moving_average(metric_data, alpha)\n",
    "    ax.plot(steps, ema_data, color=color, linewidth=4, label=f'EMA')\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_ylabel('Reward Values')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set x-axis label and ticks for the bottom subplot\n",
    "axes[-1].set_xlabel('Steps')\n",
    "max_step = max(steps)\n",
    "# Set x-ticks to show every 500 steps (which are multiples of 50)\n",
    "tick_interval = 500\n",
    "axes[-1].set_xticks(range(0, max_step + 1, tick_interval))\n",
    "axes[-1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/rewards.eps\")\n",
    "plt.savefig(\"../outputs/figures/rewards.png\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51468284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open(\"trainer_state.json\", \"r\") as json_file:\n",
    "    json_data = json.load(json_file)\n",
    "\n",
    "# Assuming json_data is already loaded, extract log_history\n",
    "data = json_data[\"log_history\"]\n",
    "\n",
    "# Filter data to only include steps that are multiples of 50\n",
    "# filtered_data = [entry for entry in data if entry['step'] % 50 == 0]\n",
    "\n",
    "# Extract steps and reward metrics from filtered data\n",
    "steps = [entry['step'] for entry in data]\n",
    "reward = [entry['reward'] for entry in data]\n",
    "confident_score = [entry['rewards/confident_score_func'] for entry in data]\n",
    "correctness = [entry['rewards/correctness_rewards'] for entry in data]\n",
    "int_reward = [entry['rewards/int_reward_func'] for entry in data]\n",
    "soft_format = [entry['rewards/soft_format_reward_func'] for entry in data]\n",
    "xmlcount = [entry['rewards/xmlcount_reward_func'] for entry in data]\n",
    "\n",
    "# Define exponential moving average function\n",
    "def exponential_moving_average(data, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Calculate exponential moving average\n",
    "    alpha: smoothing factor (0 < alpha <= 1)\n",
    "    Lower alpha = more smoothing\n",
    "    \"\"\"\n",
    "    ema = np.zeros_like(data)\n",
    "    ema[0] = data[0]  # Initialize with first value\n",
    "    for i in range(1, len(data)):\n",
    "        ema[i] = alpha * data[i] + (1 - alpha) * ema[i-1]\n",
    "    return ema\n",
    "\n",
    "# Smoothing factor for EMA (adjust as needed)\n",
    "alpha = 0.05  # Lower value = more smoothing\n",
    "\n",
    "# Colorblind-friendly palette\n",
    "cb_palette = [\n",
    "    \"#E69F00\",  # Orange\n",
    "    \"#56B4E9\",  # Sky Blue\n",
    "    \"#009E73\",  # Bluish Green\n",
    "    \"#F0E442\",  # Yellow\n",
    "    \"#0072B2\",  # Blue\n",
    "    \"#D55E00\",  # Vermillion\n",
    "    \"#CC79A7\"   # Reddish Purple\n",
    "]\n",
    "\n",
    "# List of reward metrics and their labels, using colors from cb_palette\n",
    "metrics = [\n",
    "    (reward, 'Total Reward', cb_palette[0]),\n",
    "    (confident_score, 'Confidence-alignment', cb_palette[1]),\n",
    "    (correctness, 'Correctness', cb_palette[2]),\n",
    "    (int_reward, 'Interpretability', cb_palette[3]),\n",
    "    (soft_format, 'Soft format compliance', cb_palette[4]),\n",
    "    (xmlcount, 'XML format', cb_palette[5])\n",
    "]\n",
    "\n",
    "# Create a single figure with subplots (one column, one subplot per metric)\n",
    "fig, axes = plt.subplots(len(metrics), 1, figsize=(12, 4 * len(metrics)), sharex=True)\n",
    "\n",
    "# Ensure axes is a list for iteration, even with one subplot\n",
    "if len(metrics) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Plot each metric in its own subplot\n",
    "for ax, (metric_data, label, color) in zip(axes, metrics):\n",
    "    # Plot original data faintly with dots\n",
    "    ax.plot(steps, metric_data, color=color, alpha=0.3, label=f'Raw', marker='o')\n",
    "    # Calculate and plot exponential moving average with thicker line\n",
    "    ema_data = exponential_moving_average(metric_data, alpha)\n",
    "    ax.plot(steps, ema_data, color=color, linewidth=4, label=f'EMA')\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_ylabel('Reward Values')\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(True)\n",
    "\n",
    "# Set x-axis label and ticks for the bottom subplot\n",
    "axes[-1].set_xlabel('Steps')\n",
    "max_step = max(steps)\n",
    "# Set x-ticks to show every 500 steps (which are multiples of 50)\n",
    "tick_interval = 500\n",
    "axes[-1].set_xticks(range(0, max_step + 1, tick_interval))\n",
    "axes[-1].tick_params(axis='x', rotation=90)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/figures/rewards_with_dos.eps\")\n",
    "plt.savefig(\"../outputs/figures/rewards_with_dos.png\")\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f406814",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
