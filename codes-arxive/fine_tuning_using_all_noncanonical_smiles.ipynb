{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e65305a-34e6-4dd6-92d8-1fc6f9ceb62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u111169/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "Could not load bitsandbytes native library: /share/apps/eb/Anaconda3/2023.03-1/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/u111169/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/u111169/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/home/u111169/.local/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/share/apps/eb/Anaconda3/2023.03-1/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/share/apps/eb/Anaconda3/2023.03-1/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /share/apps/eb/Anaconda3/2023.03-1/bin/../lib/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/u111169/.local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import itertools\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer , SentenceTransformerTrainer\n",
    "from sentence_transformers.losses import CachedMultipleNegativesRankingLoss ,ContrastiveLoss, OnlineContrastiveLoss\n",
    "import os\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "import torch\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from datasets import load_dataset , Dataset\n",
    "from transformers import RobertaTokenizer , RobertaForSequenceClassification , TrainingArguments , Trainer,TrainerCallback,AutoModelForSequenceClassification,AutoTokenizer\n",
    "from sklearn.metrics import precision_recall_fscore_support , accuracy_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from datasets import Dataset, Features, Value,Sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3369d79-4ecf-4676-9114-48b7e9f409af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69d7dbe2-b859-405a-a463-15577ea0c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fp(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(\"Invalid SMILES\")\n",
    "\n",
    "    # Generate Morgan fingerprint (radius=2, 2048-bit vector)\n",
    "    fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=2048)\n",
    "\n",
    "    # Convert to a bit string or list\n",
    "    bit_string = fp.ToBitString()\n",
    "    # bit_list = list(fp)\n",
    "    return bit_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6463bc47-2a1e-4a35-ac07-65b87254832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_non_canonicals(original_smiles): \n",
    "    mol = Chem.MolFromSmiles(original_smiles)\n",
    "    if mol is None:\n",
    "        print(\"Invalid SMILES\")\n",
    "    else:\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        seen = set()\n",
    "        # Generate multiple non-canonical SMILES\n",
    "        for _ in range(1000):  # Adjust iterations for more coverage\n",
    "            new_order = list(range(num_atoms))\n",
    "            random.shuffle(new_order)\n",
    "            randomized_mol = Chem.RenumberAtoms(mol, new_order)\n",
    "            randomized_smiles = Chem.MolToSmiles(randomized_mol, canonical=False)\n",
    "            if randomized_smiles not in seen:\n",
    "                seen.add(randomized_smiles)\n",
    "    return list(seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19212a99-00c9-4d59-8b4a-5bcd581471ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df2noncanonical(df): \n",
    "    f_l = []\n",
    "    for n , i in df[\"SMILES\"].items(): \n",
    "        df_c = pd.DataFrame()\n",
    "        df_c[\"SMILES\"] = get_all_non_canonicals(i)\n",
    "        df_c[\"labels\"] = df[\"labels\"][n]\n",
    "        f_l.append(df_c)\n",
    "    df_train_with_noncanonicals = pd.concat(f_l , axis=0)\n",
    "    \n",
    "    return df_train_with_noncanonicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a77aae7-847f-4b25-a53c-d1a35ca37b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports completed\n",
      "dataset created \n"
     ]
    }
   ],
   "source": [
    "print(\"Imports completed\")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "df0 = pd.read_csv(\"/home/u111169/wrkdir/mgh-project/ChemAP/dataset/DrugApp/All_training_feature_vectors.csv\")\n",
    "df=df0[[\"SMILES\",\t\"Label\"]]\n",
    "\n",
    "print(\"dataset created \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef38018b-a852-4b30-8b3f-17e87369134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42908/3489636595.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns={\"Label\":\"labels\"}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df.rename(columns={\"Label\":\"labels\"}, inplace=True)\n",
    "\n",
    "test_size = 0.2\n",
    "val_size = 0.5\n",
    "train_df , temp = train_test_split(df , stratify = df.labels , test_size = test_size , random_state=1234)\n",
    "test_df , val_df = train_test_split(temp , stratify = temp.labels , test_size = val_size , random_state=1234)\n",
    "#-----------------------------------\n",
    "# reset index\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62156db-deff-4f74-b4b1-89d9ab1675da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OC([C@@H]1[C@@H](O)[C@H](O)[C@@H](O[C@H]2[C@H]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[C@@H]12C[C@](C(=O)O)(C)CC[C@]1(C)CC[C@]1(C)C2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O1[C@H](C(=O)O)[C@@H](O)[C@H](O)[C@@H](O)[C@@H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[C@]12(C)CC[C@H](O[C@@H]3[C@H](O[C@H]4[C@H](O)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C[C@]12[C@@]3(C)C(=CC(=O)[C@@H]1[C@]1(C)[C@H](...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>[C@H]12[C@H](OC[C@H]1O)[C@@H](O)CO2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>C1[C@H](O)[C@H]2OC[C@@H](O)[C@H]2O1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>O1[C@@H]2[C@H](O)CO[C@@H]2[C@@H](O)C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>[C@@H]1(O)CO[C@@H]2[C@@H](O)CO[C@H]12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>O[C@H]1CO[C@@H]2[C@H](O)CO[C@H]12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1236019 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0   OC([C@@H]1[C@@H](O)[C@H](O)[C@@H](O[C@H]2[C@H]...\n",
       "1   [C@@H]12C[C@](C(=O)O)(C)CC[C@]1(C)CC[C@]1(C)C2...\n",
       "2   O1[C@H](C(=O)O)[C@@H](O)[C@H](O)[C@@H](O)[C@@H...\n",
       "3   [C@]12(C)CC[C@H](O[C@@H]3[C@H](O[C@H]4[C@H](O)...\n",
       "4   C[C@]12[C@@]3(C)C(=CC(=O)[C@@H]1[C@]1(C)[C@H](...\n",
       "..                                                ...\n",
       "47                [C@H]12[C@H](OC[C@H]1O)[C@@H](O)CO2\n",
       "48                C1[C@H](O)[C@H]2OC[C@@H](O)[C@H]2O1\n",
       "49              O1[C@@H]2[C@H](O)CO[C@@H]2[C@@H](O)C1\n",
       "50              [C@@H]1(O)CO[C@@H]2[C@@H](O)CO[C@H]12\n",
       "51                  O[C@H]1CO[C@@H]2[C@H](O)CO[C@H]12\n",
       "\n",
       "[1236019 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_with_noncanonicals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd009bb-a8da-4603-a909-5fe671a098d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------\n",
    "# convert the dataframes to huggingface dataset for easier upload on hub and eaiser accessibility\n",
    "dataset_train = Dataset.from_pandas(train_df)\n",
    "dataset_df = Dataset.from_pandas(val_df)\n",
    "dataset_test = Dataset.from_pandas(test_df)\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")#'FacebookAI/xlm-roberta-large'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"/home/u111169/.cache/huggingface/hub/models--DeepChem--ChemBERTa-77M-MTR/snapshots/66b895cab8adebea0cb59a8effa66b2020f204ca\")#'FacebookAI/xlm-roberta-large'\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['SMILES'], padding=\"max_length\" , truncation=True )#,max_length=166\n",
    "\n",
    "tokenized_train = dataset_train.map(tokenize_function)\n",
    "tokenized_val = dataset_df.map(tokenize_function)\n",
    "tokenized_test = dataset_test.map(tokenize_function)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = torch.tensor([len(train_df[train_df[\"labels\"] == i]) / len(train_df) for i in np.unique(train_df[\"labels\"])]).to(device)\n",
    "#class_weights = torch.tensor([0.25,0.25,0.25,0.25]).to(device)\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "        ce_loss = self.ce_loss(logits, labels)\n",
    "        l1_loss = torch.mean(torch.abs(logits))\n",
    "        return ce_loss + 0.01 * l1_loss\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss_fn = CustomLoss(class_weights)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"/home/u111169/.cache/huggingface/hub/models--DeepChem--ChemBERTa-77M-MTR/snapshots/66b895cab8adebea0cb59a8effa66b2020f204ca\" ,num_labels=2)\n",
    "model.config.classifier_dropout=0.01\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "\n",
    "from transformers import EarlyStoppingCallback\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='/home/u111169/wrkdir/mgh-project/arxive/checkpoints-arxive/fine_tuning_chemberta_classic_way/trainer-not-custom',\n",
    "    num_train_epochs= 15,\n",
    "    evaluation_strategy='steps',\n",
    "    save_strategy='steps',\n",
    "    learning_rate=2e-3,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_steps=20,\n",
    "    #save_total_limit=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    eval_accumulation_steps=1,\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    weight_decay=0.1,\n",
    "    #logging_dir = \"logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps = 10,\n",
    "    dataloader_drop_last=True,\n",
    "    save_safetensors=False,\n",
    "    adam_epsilon=1e-08,\n",
    "    warmup_steps=100,\n",
    "    seed=42,\n",
    "    lr_scheduler_type='cosine',\n",
    "    load_best_model_at_end = True,\n",
    "    label_smoothing_factor=0.01,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer = tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
