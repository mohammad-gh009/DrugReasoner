INFO 04-14 02:29:28 [__init__.py:239] Automatically detected platform cuda.
/home/u111169/mgh/grpo.py:6: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.

Please restructure your imports with 'import unsloth' at the top of your file.
  from unsloth import FastLanguageModel
🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.
Unsloth: Failed to patch Gemma3ForConditionalGeneration.
🦥 Unsloth Zoo will now patch everything to make training faster!
/home/u111169/mgh/grpo.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.
The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.

For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.


  df["labels"].replace({"<APPROVED>":"approved" , "<NOT APPROVED>":"unapproved"} , inplace=True)
==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.1. vLLM: 0.8.3.
   \\   /|    Tesla V100-PCIE-32GB. Num GPUs = 1. Max memory: 31.739 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.6.0+cu124. CUDA: 7.0. CUDA Toolkit: 12.4. Triton: 3.2.0
\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Unsloth: Your GPU does not support prefix caching - will disable!
Unsloth: vLLM loading /home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659 with actual GPU utilization = 59.35%
Unsloth: Your GPU has CUDA compute capability 7.0 with VRAM = 31.74 GB.
Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 5000. Num Sequences = 160.
Unsloth: vLLM's KV Cache can use up to 3.8 GB. Also swap space = 6 GB.
WARNING 04-14 02:29:36 [config.py:2704] Casting torch.bfloat16 to torch.float16.
INFO 04-14 02:29:43 [config.py:600] This model supports multiple tasks: {'generate', 'embed', 'classify', 'score', 'reward'}. Defaulting to 'generate'.
WARNING 04-14 02:29:43 [arg_utils.py:1708] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
Unsloth: vLLM Bitsandbytes config using kwargs = {'load_in_8bit': False, 'load_in_4bit': True, 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_quant_storage': 'uint8', 'bnb_4bit_quant_type': 'fp4', 'bnb_4bit_use_double_quant': False, 'llm_int8_enable_fp32_cpu_offload': False, 'llm_int8_has_fp16_weight': False, 'llm_int8_skip_modules': [], 'llm_int8_threshold': 6.0}
INFO 04-14 02:29:43 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='/home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', speculative_config=None, tokenizer='/home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=LoadFormat.BITSANDBYTES, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=bitsandbytes, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":0,"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":160}, use_cached_outputs=False, 
/share/apps/eb/Python/3.11.5-GCCcore-13.2.0/envs/vllm/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py:29: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
  get_ip(), get_open_port())
INFO 04-14 02:29:44 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.
INFO 04-14 02:29:44 [cuda.py:289] Using XFormers backend.
[W414 02:29:44.161060490 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3
[W414 02:29:44.161288004 socket.cpp:204] [c10d] The hostname of the client socket cannot be retrieved. err=-3
INFO 04-14 02:29:44 [parallel_state.py:957] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0
INFO 04-14 02:29:44 [model_runner.py:1110] Starting to load model /home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659...
INFO 04-14 02:29:44 [loader.py:1155] Loading weights with BitsAndBytes quantization. May take a while ...
Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.79it/s]
Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.07it/s]
Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.69it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.59it/s]
Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.75it/s]

INFO 04-14 02:29:47 [punica_selector.py:18] Using PunicaWrapperGPU.
INFO 04-14 02:29:47 [model_runner.py:1146] Model loading took 5.4442 GiB and 2.531060 seconds
INFO 04-14 02:29:58 [worker.py:267] Memory profiling takes 10.58 seconds
INFO 04-14 02:29:58 [worker.py:267] the current vLLM instance can use total_gpu_memory (31.74GiB) x gpu_memory_utilization (0.59) = 18.84GiB
INFO 04-14 02:29:58 [worker.py:267] model weights take 5.44GiB; non_torch_memory takes 0.07GiB; PyTorch activation peak memory takes 0.77GiB; the rest of the memory reserved for KV Cache is 12.55GiB.
INFO 04-14 02:29:58 [executor_base.py:112] # cuda blocks: 6426, # CPU blocks: 3072
INFO 04-14 02:29:58 [executor_base.py:117] Maximum concurrency for 5000 tokens per request: 20.56x
INFO 04-14 02:30:01 [model_runner.py:1456] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
Capturing CUDA graph shapes:   0%|          | 0/23 [00:00<?, ?it/s]Capturing CUDA graph shapes:   4%|▍         | 1/23 [00:01<00:34,  1.58s/it]Capturing CUDA graph shapes:   9%|▊         | 2/23 [00:03<00:31,  1.49s/it]Capturing CUDA graph shapes:  13%|█▎        | 3/23 [00:04<00:29,  1.46s/it]Capturing CUDA graph shapes:  17%|█▋        | 4/23 [00:05<00:27,  1.45s/it]Capturing CUDA graph shapes:  22%|██▏       | 5/23 [00:07<00:25,  1.39s/it]Capturing CUDA graph shapes:  26%|██▌       | 6/23 [00:08<00:23,  1.36s/it]Capturing CUDA graph shapes:  30%|███       | 7/23 [00:09<00:21,  1.34s/it]Capturing CUDA graph shapes:  35%|███▍      | 8/23 [00:11<00:19,  1.33s/it]Capturing CUDA graph shapes:  39%|███▉      | 9/23 [00:12<00:18,  1.32s/it]Capturing CUDA graph shapes:  43%|████▎     | 10/23 [00:13<00:16,  1.31s/it]Capturing CUDA graph shapes:  48%|████▊     | 11/23 [00:14<00:15,  1.30s/it]Capturing CUDA graph shapes:  52%|█████▏    | 12/23 [00:16<00:14,  1.29s/it]Capturing CUDA graph shapes:  57%|█████▋    | 13/23 [00:17<00:12,  1.25s/it]Capturing CUDA graph shapes:  61%|██████    | 14/23 [00:18<00:10,  1.21s/it]Capturing CUDA graph shapes:  65%|██████▌   | 15/23 [00:19<00:09,  1.18s/it]Capturing CUDA graph shapes:  70%|██████▉   | 16/23 [00:20<00:08,  1.16s/it]Capturing CUDA graph shapes:  74%|███████▍  | 17/23 [00:21<00:06,  1.15s/it]Capturing CUDA graph shapes:  78%|███████▊  | 18/23 [00:22<00:05,  1.14s/it]Capturing CUDA graph shapes:  83%|████████▎ | 19/23 [00:24<00:04,  1.14s/it]Capturing CUDA graph shapes:  87%|████████▋ | 20/23 [00:25<00:03,  1.13s/it]Capturing CUDA graph shapes:  91%|█████████▏| 21/23 [00:26<00:02,  1.12s/it]Capturing CUDA graph shapes:  96%|█████████▌| 22/23 [00:27<00:01,  1.12s/it]Capturing CUDA graph shapes: 100%|██████████| 23/23 [00:28<00:00,  1.00s/it]Capturing CUDA graph shapes: 100%|██████████| 23/23 [00:28<00:00,  1.22s/it]
INFO 04-14 02:30:29 [model_runner.py:1598] Graph capturing finished in 28 secs, took 3.07 GiB
INFO 04-14 02:30:29 [llm_engine.py:448] init engine (profile, create kv cache, warmup model) took 41.86 seconds
/home/u111169/wrkdir/mgh-project/models/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659 does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.
Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.
Map:   0%|          | 0/2497 [00:00<?, ? examples/s]Map:  66%|██████▌   | 1641/2497 [00:00<00:00, 16324.16 examples/s]Map: 100%|██████████| 2497/2497 [00:00<00:00, 15051.99 examples/s]
WARNING:accelerate.utils.other:Detected kernel version 5.3.18, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1
   \\   /|    Num examples = 2,497 | Num Epochs = 1 | Total steps = 10
O^O/ \_/ \    Batch size per device = 4 | Gradient accumulation steps = 1
\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4
 "-____-"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)
  0%|          | 0/10 [00:00<?, ?it/s]`generation_config` default values have been modified to match model-specific defaults: {'max_length': 131072, 'bos_token_id': 128000, 'eos_token_id': [128001, 128008, 128009]}. If this is not desired, please set these values explicitly.
--------------------
Question:

            I have developed a model that can predict the drug approval of compound X.
            This model outputs two lists containting the properties of most similar approved and unapproved modelcules
            Your task is to analyze the likelihood of compound X receiving regulatory approval based on its similarity to known molecules.

            - RDKit Analysis of Compound X:
            {'Molecular Weight': 467.65, 'LogP': 4.41, 'Molecular Refractivity': 130.2, 'TPSA': 62.16, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 5, 'Rotatable Bonds': 4, 'Chiral Centers': 7, 'Heavy Atoms': 34, 'Formal Charge': 0, 'Total Rings': 8, 'Structural Alerts': 'None'}

            Using the provided data:
            {'most similar appoved drugs': {1: { 'similarity score': 0.99, 'Rdkit information': {'Molecular Weight': 504.11, 'LogP': 4.84, 'Molecular Refractivity': 137.45, 'TPSA': 62.16, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 5, 'Rotatable Bonds': 4, 'Chiral Centers': 7, 'Heavy Atoms': 35, 'Formal Charge': 0, 'Total Rings': 8, 'Structural Alerts': 'None'}}, 2: { 'similarity score': 0.99, 'Rdkit information': {'Molecular Weight': 467.65, 'LogP': 4.41, 'Molecular Refractivity': 130.2, 'TPSA': 62.16, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 5, 'Rotatable Bonds': 4, 'Chiral Centers': 7, 'Heavy Atoms': 34, 'Formal Charge': 0, 'Total Rings': 8, 'Structural Alerts': 'None'}}, 3: { 'similarity score': 0.93, 'Rdkit information': {'Molecular Weight': 339.44, 'LogP': 2.51, 'Molecular Refractivity': 93.93, 'TPSA': 52.93, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 4, 'Rotatable Bonds': 2, 'Chiral Centers': 4, 'Heavy Atoms': 25, 'Formal Charge': 0, 'Total Rings': 6, 'Structural Alerts': ['isolated_alkene']}}, 4: { 'similarity score': 0.9, 'Rdkit information': {'Molecular Weight': 357.45, 'LogP': 1.71, 'Molecular Refractivity': 95.41, 'TPSA': 73.16, 'Hydrogen Bond_Donors': 3, 'Hydrogen_Bond Acceptors': 5, 'Rotatable Bonds': 2, 'Chiral Centers': 5, 'Heavy Atoms': 26, 'Formal Charge': 0, 'Total Rings': 6, 'Structural Alerts': 'None'}}, 5: { 'similarity score': 0.89, 'Rdkit information': {'Molecular Weight': 356.44, 'LogP': 1.67, 'Molecular Refractivity': 94.14, 'TPSA': 66.76, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 4, 'Rotatable Bonds': 2, 'Chiral Centers': 4, 'Heavy Atoms': 26, 'Formal Charge': 1, 'Total Rings': 6, 'Structural Alerts': ['quaternary_nitrogen_2']}}}, 'most similar non appoved drugs': {1: { 'similarity score': 0.89, 'Rdkit information': {'Molecular Weight': 476.57, 'LogP': 3.09, 'Molecular Refractivity': 129.09, 'TPSA': 86.38, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 6, 'Rotatable Bonds': 5, 'Chiral Centers': 5, 'Heavy Atoms': 35, 'Formal Charge': 0, 'Total Rings': 7, 'Structural Alerts': ['Michael_acceptor_1']}}, 2: { 'similarity score': 0.86, 'Rdkit information': {'Molecular Weight': 513.03, 'LogP': 3.51, 'Molecular Refractivity': 136.34, 'TPSA': 86.38, 'Hydrogen Bond_Donors': 2, 'Hydrogen_Bond Acceptors': 6, 'Rotatable Bonds': 5, 'Chiral Centers': 5, 'Heavy Atoms': 36, 'Formal Charge': 0, 'Total Rings': 7, 'Structural Alerts': ['Michael_acceptor_1']}}, 3: { 'similarity score': 0.85, 'Rdkit information': {'Molecular Weight': 469.62, 'LogP': 4.88, 'Molecular Refractivity': 126.17, 'TPSA': 49.39, 'Hydrogen Bond_Donors': 0, 'Hydrogen_Bond Acceptors': 6, 'Rotatable Bonds': 5, 'Chiral Centers': 0, 'Heavy Atoms': 34, 'Formal Charge': 0, 'Total Rings': 8, 'Structural Alerts': ['peroxide']}}, 4: { 'similarity score': 0.85, 'Rdkit information': {'Molecular Weight': 277.5, 'LogP': 5.3, 'Molecular Refractivity': 86.9, 'TPSA': 12.03, 'Hydrogen Bond_Donors': 1, 'Hydrogen_Bond Acceptors': 1, 'Rotatable Bonds': 4, 'Chiral Centers': 0, 'Heavy Atoms': 20, 'Formal Charge': 0, 'Total Rings': 3, 'Structural Alerts': 'None'}}, 5: { 'similarity score': 0.85, 'Rdkit information': {'Molecular Weight': 321.8, 'LogP': 2.05, 'Molecular Refractivity': 83.92, 'TPSA': 49.77, 'Hydrogen Bond_Donors': 1, 'Hydrogen_Bond Acceptors': 4, 'Rotatable Bonds': 0, 'Chiral Centers': 4, 'Heavy Atoms': 22, 'Formal Charge': 0, 'Total Rings': 5, 'Structural Alerts': 'None'}}}}
            Answer:
approvedResponse:
<think>

To analyze the likelihood of compound X receiving regulatory approval, I will compare its properties with those of the most similar approved and unapproved molecules.

The approved molecules have a high similarity score (0.99 and 0.93) with compound X, indicating that they share many properties. Specifically:

* Both approved molecules and compound X have a high molecular weight (467.65 and 504.11) and a moderate to high number of hydrogen bond acceptors (5 and 6).
* They also have a moderate to high number of rotatable bonds (4 and 5), which can contribute to their bioavailability and pharmacodynamics.
* Additionally, both approved molecules and compound X have a similar number of heavy atoms (34 and 35), which can affect their stability and toxicity.

On the other hand, the unapproved molecules have a lower similarity score (0.89, 0.86, 0.85, and 0.85) with compound X, indicating that they share fewer properties. However, some of these unapproved molecules also have properties that are similar to those of the approved molecules, such as:

* A moderate to high number of hydrogen bond acceptors (6 and 7).
* A moderate to high number of rotatable bonds (5 and 6).
* A moderate to high molecular weight (476.57 and 513.03).

Overall, the approved molecules appear to have properties that are more similar to those of compound X, suggesting that it may have a higher likelihood of approval.

Considering the properties of the approved and unapproved molecules, I would estimate the likelihood of compound X receiving regulatory approval as follows:

1. The probability of compound X being approved is the product of the probabilities of its individual properties being favorable, which can be estimated as follows:

   * Probability of high molecular weight: 0.9 (similar to approved molecule 1)
   * Probability of moderate to high number of hydrogen bond acceptors: 0.9 (similar to approved molecule 1)
   * Probability of moderate to high number of rotatable bonds: 0.8 (similar to approved molecule 1, with some variation)
   * Probability of similar number of heavy atoms: 0.8 (similar to approved molecule 1, with some variation)

   The overall probability of compound X being approved would be the product of these individual probabilities: 0.9 * 0.9 * 0.8 * 0.8 = 0.5184

   However, this is a very rough estimate and should be taken as a rough order of magnitude rather than a precise probability.

2. The confidence level of this estimate is difficult to quantify, but I would rate it as follows:

   * Low confidence: 0.0 (completely uncertain)
   * Moderate confidence: 0.2 (some uncertainty)
   * High confidence: 0.5 (reasonable confidence)
   * Very high confidence: 0.8 (quite confident)
   * Extremely high confidence: 1.0 (very confident)

   Based on the analysis above, I would rate the confidence level of this estimate as 0.5 (moderate confidence). This suggests that there is some uncertainty in the estimate, but the analysis suggests that compound X may have a reasonable chance of approval.

</think>

<label>approved</label>

<score>0.5</score>Extracted:
approvedScore:
0.5






[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/u111169/mgh/grpo.py", line 224, in <module>
[rank0]:     trainer.train()
[rank0]:   File "/share/apps/eb/Python/3.11.5-GCCcore-13.2.0/envs/vllm/lib/python3.11/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:            ^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "<string>", line 314, in _fast_inner_training_loop
[rank0]:   File "<string>", line 25, in _unsloth_training_step
[rank0]:   File "/share/apps/eb/Python/3.11.5-GCCcore-13.2.0/envs/vllm/lib/python3.11/site-packages/trl/trainer/grpo_trainer.py", line 626, in _prepare_inputs
[rank0]:     output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)
[rank0]:                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/u111169/mgh/grpo.py", line 146, in confident_score_func
[rank0]:     if r == a and s >= 0.7: 
[rank0]:                   ^^^^^^^^
[rank0]: TypeError: '>=' not supported between instances of 'str' and 'float'
[rank0]:[W414 02:32:20.992787739 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
  0%|          | 0/10 [01:42<?, ?it/s]
#################################################
Panthera Cluster
Job 110559 for user 'u111169'
Finished at: Mon Apr 14 02:32:23 +0330 2025
Job details file : ~/JobSummary/110559.out
================================================
