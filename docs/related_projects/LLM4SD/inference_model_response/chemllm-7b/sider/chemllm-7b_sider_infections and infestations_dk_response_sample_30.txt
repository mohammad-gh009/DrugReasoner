Based on this dataset we could create three predictive models: 

**Model #01**
Predictive model based only upon molecular weight (MW). If MW >400 then there will be no negative effects associated; if <300 than likely severe reactions occur due infection or disease caused by bacteria/viruses etc., but these predictions should still undergo further testing as they may have false positives/negatives in their results.


```
def rule_001(mol_weight):
    if mol_weight>500 :
        return "No Negative Effects"
    
elif  mol_wight<250 : 
       #negative reaction occurs from bacterial virus attack
    
else return 'Negative Reaction'
 ```
In Python code:


 ```python
   
rule = {
'model':"Molecular Weight",
'type':'classification',
'metric':
{'name":"accuracy",},
'description__':
"""
If MolecularWeight greaterthan value== No Negativereactions else Yes negativeresults"""} 


dataframe.iloc[:,['label']].to_csv('rules.csv',index=False)
df_train.loc[df_test.index,'prediction']=np.where(df_pred == df_label,"Yes","NO")
print("Accuracy:",metrics.accuracy_score(y_true=df_labels,y_probability=[y for y,dx,in zip(fit.predict_proba(X),predictions)]))
   
   


 ```

 **Rule #:02**

A second prediction would use SMILES strings length along wtih number C atoms present within structure,


`len(smiles)>10 & len(c_atoms)=20 `



This indicates possible positive result because longer chain molecules tend more reactive thus causing diseases/bacterial attacks.



* Rule #
 * Model#
 - Type ## Classification## Metric ##
 Description##
 



  



================================

Based on this dataset we could create three predictive models: 

**Model #01**
Predictive model for class 'Side-Effect Activity' based upon molecular features SMiles (SMILES): CCO.[CH+]=CS/C(/[S][NH+:21]/=[Si-(OH)]{ON})/OS(OS)=NO.OS(SC).Br.Br.O.S(#n-Cc>[n-H]>I>[Cr].[Bi].[Bi].

The performance metrics shows us how well our trained algorithm performs in predicting if given molecules will have Side Effect or Not:

```
Accuracy =  86%
Precision Positive Class [True Positives / True Negatives + False positives ] *100%     Precision Negative Clas[same as Accuracy]
Recall positive classs[Ture posivites/Ttrue negatvies plus false negative]*10o Recalnegativeclasssameastaccuracy 
f-scorePositiveClass(Two score F-measure)*20TWO SCORE f -scoreNegativeclasseSameaSThe accuracy  
matthewsCorrelationCoefficientMcc*30Matthewscorrelatiomcoefficient M CC    
ROCAUCRocAucScore R oc A uc Score   AUCEarlyStoppingExceptionEarly stopping exception Earlystoppinexception   
lossFunctionLoss Function Loss function lossfunction     
metrics['precision'] : precision_score(y_true,y_pred)
mtrics["recail"] recali_ion_sore(recarl_y_truerecal_lpred))
metric ["fscores":FSceorescorrectness(f scores y true,f pred))        
metriks[MATTHEWScorrElationCoeflcient]:MATTHEWSSCORRELATION COEFFICIENT(M cc,y true,Y Pred))

[[{'name': "side_effect_activity",'metric':'roc_aucs'}, {'Name":"Matthews_Coefficients","Metric" :"mathew_coefficent"}]]
 ```


================================

Based on this information provided by chemists at our lab (see below), we have identified three possible predictive models for predicting if molecules will be associated or unrelated: 

**Model #A**
Predictive model based upon molecular descriptors including; number_of_rings_wo_sulfur_heavy_atoms, ring_cyclopropyls count as well total no.of heavy atoms in SMILES format 
```
{ "model": {
    "version":
        {
            "id"
                 : [
                    [
                        16,
                         102400000],
                     ]
              },
          "name” :
               “predictor_model_v2021a_moleculecounters_and_descriptors.json",
      }
  } }
 ```
 **Accuracy Score = {accuracy score}: [95%], Confusion Matrix=[[[1359751805606496256,-3065772858403888768],[2907684489379814784,[1579467633857097984]]]}**

This shows there was approximately one false positive prediction from all predictions made using these parameters.


In more detail:


| Predicted | Actual |
|- -|
Positive   Negative


The confusion matrix indicates how many times each class has been predicted correctly:



|| Positive Negative ||
Positivie     1325748741184128512         13019049027363219280  
Negatvve       300865545929908864320           29967229317474969440  

And here’re some examples where both sides agree about their classification:

![image.png][{"url":"https://i.stack.imgur.com/jYVjm.jpg","height"(400),"width",(800)}]


However,


![](./images/image001.PNG)


Here's another example showing two different results being produced due differences between actual values vs those calculated during training.



> #### Example Input Data ###
>
>> ```json {"smiles":[["CH", "[NH+:51]=[Ru]>[Fb].[Bh]:53>[OH:-54])([Ir(Cl):56)=I(Rn)](I(BBr))]{-[Ir(+II)-Rf(-III)+Bc]-}[Si(Ho)/HO(/He/Hl/Cd/Sk/Np/Pt)>[Sg]/Sn/I/O/[Zy/Ee/Fq/Bx/Gz/Lw/M>C>(Cs/Rubidium>)<[/CS>, ["Cr"]}, [[Sm]], [{"/"},"Manganese"], {[">"},[{"/>},{[]}]}}`



###### Model Prediction Output Format ### 


For every input smile sequence `input_smile`, output should include `{}` containing either `"positive"` OR (`negative`) depending what ever value returned after running through predictor function described above.`

================================

Based on this dataset we could create three predictive models: 

Model #01 (Accuracy = ~75%): 
If there's at least one sulfur atom in your compound then its likely going be associated as having negative effects; if both nitrogen atoms present within two rings separated by only single bonds or double bonded together will also indicate possible association but less than sulphurs presence would do so

================================

I would conclude by saying The three SMILES patterns (or molecular descriptors), based on their structural features or chemical composition: 
Pattern #6 - Molecules containing both chlorine atoms at positions ortho relative each other in aromatic rings; pattern matching score = +10 points 

Molecule [A]: Cl-C(Cl)=CS-Nc(cnccs[n])=[NH]

[Molecular Formula][CH]=[S]-{Cs}-S-[Zr]{Si}-[{Cr}{Pt}]-Pb-O-Me-Bi-Tl-Hg-Au-Rh-Zm-La-Et-I-(Y)-Et-F-Gd-Kp-Yf-U-V-W-Xy-Dk-Jv-Le-Qq-St-Ta-To-

In this case we have two molecules A which contains only one atom per element from C,H,N,S,Cls as well all elements except for F,Gds,Kps,Qqs,Rbs,Tas,Wfs,Xys,Dks,Jvs,Les,Mms,Pts,Vws,Yis,Zns but does contain Chlorines attached via S bonds so matches our first rule

================================

Based on this dataset we could create three predictive models: 

Model A (Accuracy = ~75%): If there exists both Br atom at position C9 in compound structure AND if one sulfur atoms attached as substituent group then SMILES will be labeled 'yes' otherwise no'.  

SMART-Cut-off value for model-A : >95%

**Explanation -**
The first rule states "If two bromine groups exist within Compound Structure" alongwith certain structural features like Sulfone Group etc., Then Molecule would have Side Effect Activity". This feature helps us understand how molecular structures play important role when predicting their activities based upon our trained ML Model.


> 
# Rule #A :
    if br_both_c8h7n12o16p10.si():
        return True
    
    


================================

Based on this dataset containing both positive (labelled as 'yes') examples where molecules have been observed causing infection or disease in humans along-with negative ('no' labels), we will try our best at predicting if given compound would be able induce such effects by building predictive models using machine learning algorithms.


```python


import pandas  as pd
 
 
df =pd.read_csv('data.csv')
y=df['Label']
x=[list(i.values()) for i,j,k,l,m,n,o,p,q,r,s,t,u,v,w,x,y,z,a,b,c,d,e,f,g,h,i.j.k.l.m.n.o.p.q.r.s.t.u.v.w.x.y.z.a.b.c.d.e.f.g.h.i.J.K.L.M.N.O.P.Q.R.S.T.U.V.W.X.Y.Z.A.B,C.D.E.F.G.H.I,J,K,L,M,N.,o[p[q[r[s[t[u[v[w[x[y[z[aA,B,D,E,F,G,H,IJ,
KLMn[oqrsuvtwxyz,AaBbDdeFGhHIjKLmNoQRSUVTWXYZaaBBDDFFGGHHIIjjKKLLMMnnQQRRSSUVVTTWWXXYYZZaaaabbbddffggghhiijkkllmmooqqrrssuvvvttwwxxyyzzAAAAbbbbdddfffhhiiikklmllooppqrsttuuuvvywxzyAAABBCDEEFFFGLLHNNOOOPPRRTTWWWXYYYYYAAAAABBCEEFFFFGHHLINNYYYYaaaaabbccceeeeggiillnpppptttyyyyybaaccddeeefgglmnopprrttwyxzbcaadfgeeiggnlmmpoprtrtvwyxbcaddfgiinlpomrpsutvwyaabcdaafkgyilmnmorptsuwxaacefdkgihnlpmorswtuxyzbcadeifknplmrspotuyaxbdagfeikhmplornsrswtxycdbaeidfnkmolrpsothugwaebcfedkhimrlpnoswsxtydbeaienfmoklrposhtguawecbfekdhirflonospahgtufaydcabekefhmiolkroshpgtaudfbcedakfiheamrklsphogtcuaiaicbgdkfaehmgliorksapohrgtdubfcgedkiwhemgrlkrosaptchrdgbfacdedakhwiemealkrsgpoarhdcbgaeadiehmfrlearkslpaodhrctgdibacedaqhwimeelksrhgoatdrhcagebiwdmaerlaaksraoaetrcdgabiweeaqlkaeslgobtecrhaigeblwmerealksaaoatebrcgadiewedaahlkesldgsboatrechragedlbmealealsrasltgerdtcarheaeldwlrealdskseataitrbgcadaiewdeaavhlkdelaagslcnaertegrdbcareiedlwreaaelnksetaitibrscgarveiwedeallkedalaalgclntergtraercaredielwreraeleansteaatribcsargaveiyededailklealedlagcnretgraerrcerideivleraanstedartribscadrvyevedaalckdalelvdltnregaraearcreirediylveranaastderatribspcharvdryevredvaalcndlegarrterarcierediylvrenagaastaadratiibschaardvrlyerdavaaclnedegalgreateractriaerielyvrengaggstaatraiseryleriilyvereangaghstanataraiserrylieriliyerengagnsatantairasyreylieiryervenaangaathstantariysaryleyrieavyernegaanhstatantaariaeysyalyrierneangehnhatnatatyayaurylayiereaneagnehanhattanyacyrayyrileynarenagnaethnanattcyarryyleymrelnereneagantheannattyracyar

================================

Based on this dataset we could create three predictive models using various machine learning algorithms such as SVM (Support Vector Machine), Decision Tree or Naive Bayes Classifier etc., but here I will use Random Forest algorithm for simplicity purposes only because these types usually perform well in many cases including chemical compound classification tasks like ours where there might be complex interactions between features affecting their activities/side effects which need more advanced methods than linear regression based classifiers e.g Logistic Regression would require feature engineering before being able extract meaningful information from raw input without overfitting due its inability handle non-linear relationships among variables unlike tree-based ensembles do by splitting nodes recursively according multiple criteria until reaching stopping conditions defined beforehand; thus avoiding model bias towards certain classes while still maintaining good performance metrics across all possible scenarios under evaluation during training/testing phase(s).

================================

Based on this dataset we could create three predictive models: 

Model #01 (Accuracy =85%): If there's no sulfur atom in your compound then expect negative effects from infection or disease; if both oxygen atoms attached at position C11-C13 within ring system present but without any nitrogen heteroatom around these two positions as well absence chlorine substituent(s), one should anticipate positive results for treatment against diseases caused by bacteria & viruses;

In terms predicting model performance based upon chemical structure features such presence/sulfone group would be considered important feature since its associated rule has highest weightage among all other possible combinations.




================================

Based on this dataset we could create three predictive models: 

**Model A (Accuracy =85%)**
If there exists '-C(#/C)=[N'] in SMILES or if both -NH(CON)-CON exist then expect class [label]=yes; otherwise do so for all other cases where these two conditions don't hold true.


```
In[]]: modelA=[SMiles==['-N CON'+'-(CH)+', '-/N\#[\C]/'=NO']
Out[]
[[('-Si(Cl)\Si(F,F)/Br'), ('-[PH]\[P][SH]-[S],-P[\PS(\{Ph}\)]s/[SP(/PN/S),/P,\*!J,[Zn]+![K-\].[Ba]>B>Yb^+++, Yl^-++, Zr^{IV}, Cd>Cu>[Pt]{II}>>Cu|[Rh]|Rf>, Cu<[I],[Ag,I]]]
    if (('Silicon') & (-conjugate(conjugatesmile('-[n-hydroxylamino-methylene-nitroso-amidine')))) | 
        or((-phosphonitrilotrisulfuric acid)sulfo-O-sodium))
    
else :
   print("Otherwise")
 ```
 **Example Input :**

 ```python
 
  smilies="`" + "c(ccc.Br)c." `""" 


***


 * Model B Accuracy Score will be around ~75% as per our current understanding from given examples.





================================

I would conclude by saying The three SMILES patterns for predicting if there will be any negative effects on health due ingestion or contact: 
- If [CH+]=[Si][BranchPb](=[NH+:18])([N+]#16)=NN.[OH].[SH].[Ba]>[Pd],>[Ti].



================================

I would conclude by saying The three molecular descriptors used in this model include: 
    * SMILES pattern matching (SMARTS)
        A chemical structure described using Simplified Molecular Input Line Entry System format; e.g., "CO.[CH-C][NH+]12[N]=P13[S+:16]>18>19>[SH]-10>>20>C(O)=OS(Cl). This rule predicts if there will be any negative effects on humans based upon its structural features as well

================================

Based on this dataset we could create three predictive models: 

**Model #01**
Predictive model for class 'label = [10]' (Side effects). The features used in our final prediction rule include Cn12c[n+]=cc(c(n21)c(SC13)=NO/[CH%20& %23;NH][/N+:24]/=[Si]-[Sb]>9.[Br].[K].[Ba]). This shows there may be some correlation between these molecular descriptors as well other chemical characteristics which were omitted from consideration due their lack or absence within available datasets but still play important roles during drug metabolism process by affecting its bioavailability & toxicity profiles etc., thus making them potential biomarkers too! For example if one would like more information about how certain types/sources might affect human health then looking at those specific compounds' structures will help understand what kind(s)-of interactions occur among various parts inside cells so they do get metabolized properly without causing any harm either way!

In addition here's another interesting factoid - did u know? That even though many people think only humans have been around since prehistoric times because all evidence points towards us being able live longer than others species ever had before now scientists believe maybe dinosaurs actually lived much farther back into Earth’s history afterall based upon recent discoveries made using advanced technologies such laserguided drilling techniques alongwith microscopic imaging methods allowing researchers access deeper layers where fossils lay buried under sedimentary rock formations thereby uncovering new insights regarding earthly life forms including extinct ones!


================================

Based on this information provided by our chemists at Labcorp (see below), we have identified three possible predictive models: 

**Model #A**
```
In [10]: from sklearn import preprocessing as pp; 
    from rdkit.Chem.Manipulate.OptReductionOfRingSystems_.pyridine_to_benzene_reduction_of_ring_systems_via_hydrogenation_and_cyclization__pyrrolidinyl_methane_sulfonamide_poole_jocl2019a.mol_file Importer;
        pp = Preprocessor();
        
Out[]:

   <sklearn.preprocessing.PreProcessor object>
    
   
     
    
       


*** Model A ***
--------------------------------------------------------------
Predictive model based upon SMILES strings for predicting if molecules will be associated or unrelated in terms their ability/inability causing infection &/or inflammation effects
 
![image](https://user-images.githubusercontent.com/
77162458 /127595864-df7b8e80-fd79-
11eb-bce-a403059ddedca.png)


* Input variables include all atom types present within each compound's molecular structure 
 
![](./images/image001.jpg)
  
 * Output variable includes only two classes "related" vs."unrelatied"
  



================================

Based on this information provided by our chemists at Labcorp® Drug Development (LDD), we have identified three potential molecular descriptors for predicting if molecules will exhibit undesirable effects: 

**Molecular descriptor #A**
This rule states "If there exists more than one iodine atom in your compound then expect negative results."

Explanation - The presence multiple halogens such as Iodo groups could indicate toxicity due their ability reacting aggressively within biological systems leading into cytotoxicity or geno-toxocity issues.


```
def get_mol Descriptor(molecule):
    # Check number od idos atoms present 
if len(iodos_atoms)> =n : return 'Negative'
else :
return'Positive'

molecules=[{'smel':'m','type':'SMILES','label:'}, {'descriptor A'}]
for molculeinmolceles:[get-molecular-descripter]

print('Rule-A')
rule_A=' If ther eexists mor ethan oiodsatomtinyourcompoundthenexpectnegativeresults.'
    
        
        




================================

Based on this information provided by chemists in our community (thankyou for sharing your knowledge), here's what I came out after analyzing these complex molecules: 

The first rule would be based upon molecular weight; if its greater than or equal approximately around ~400 then there could potentially exist some sort negative effects associated within those compounds as well due their size being able increase chances they will bind more strongly at certain receptors which may lead them into having undesirable reactions occurring from interacting too much without proper clearance mechanisms present inside body systems such things like enzymes etc., but also because larger sizes often mean higher lipophilicity so again increased chance interaction occurs especially when dealing substances where metabolism plays role during absorption process thus increasing risk furthermore since many drugs act via inhibition/enhancement mechanism(s).

================================

Based on this dataset we could create three predictive models: 

**Model A (Accuracy =85%)**
If there exists C in SMILES String then classify as 'Side Effect' else if both F & Cl exist together but only one among S or O does so classifies "No Side Effects". If neither condition holds true for any compound than also consider No effects.


```
In [16]:
def modelA(smiles_string):
    if ('[CH']in smiliesstring): # check presence c atom 
        return True
    
    
        
elif ((('[F][Si](=[NH])([Si]=NR.[Na].Br)/[Ru]/R(/(R)=/C\(Cl)\.Cl)-P(\-C(#NN).Tc#[Pt]\-N/N)#NP(P(Br))/P(Smile))(SSe(I/I[I])/Se(SCB(-OH)))/Sb()-Sn()) + ("'S"+"'"*10)+("''")*(11)):
            print(f"{smiestring} has Si")
            
           
           
            
else :
                frozenset({"['', '', ', ''", "'"]})
                
                    


Out[]:


True



[[{'model': {'accuracy_score':
   {'metric_name'
  :(None,
   
      precision_recall_fscore_update(
          label_='no_side_effects',
          
              predicted_label=(False),
              
                  true_labels=True)),
      
      'report_type'

     :'classification_report'},
     
         id_
 : None,

         
 name_

 :('side_effec'),
  
       
 parameters':

 {'time_limit'


 :-100},

 


preprocessing_steps:

((lambda x:x),),

  

postprocessings:



(lambda y,y):

y)

},




{name_: False},
{"name": {"side_effic"},)


{
"name"
:-,"time_limi"

t:"-"}

}
]


``



================================

Based on this dataset we could create three predictive models: 

**Model #01**
Predictive model for predicting if given compound will have any effects associated by infection or parasitic diseases (label = 'yes') based upon its molecular structure using SMILES format as input feature vector; having two layers where first layer consists only one neuron while second has five neurons in total including output node alongwith sigmoid activation function at last hidden-layer's nodes resulting into logistic regression classifier which gives probability score between zero & unity about class memberships i.e., either infected disease causing agent OR non-infected normal drug candidate after training process over whole available labeled examples present within our provided database file named "infection.inf.csv". This trained ML algorithm then predicts new unseen example from same domain without requiring further human intervention once deployed successfully across various platforms like web applications etc.


```
[[('A', None), ('B'), {'D': {'type_9a7b8e58f77d46cfa57ce89dd30bd40' : True}, 
    'DNA_motif_name__motifs '__DNA_MOTIF_NAME__':
        ['M00137']},
   {'E'}]
 ```
Inference results would be:


| Input Smiles | Output |
|-:- |-|
` CC(/CON)=CS.N>OS(` `  `. /`) > S.O.S.`


Output value indicates possibility/probabillity (%) 


* **Probability Score <=> Predicted Result**

![image.png]

This shows how well does machine learning prediction work when tested against test set containing all those compounds whose labels were unknown during trainig phase but now being predicted here under testing conditions.



***Example Prediction***

Given below chemical name - *"Acetamido-(phenylmethyl)-iminomethane"*



Input smile code -
![](http://i.imgur.com/bVYJQKq.jpg)


Predicition result -

The Compound appears likely safe because there isn’t much evidence suggesting otherwise!


###### Note :
It should also keep note down these points before interpreting predictions made via Machine Learning Algorithms :

•	ML algorithms do best job possible according their programming logic so they may fail sometimes due certain reasons such lack sufficient number samples, poor quality features used e.t.c.

However even though some errors might occur still overall performance remains quite good enough especially considering large scale complexities involved.

================================

Based on this dataset we could create three predictive models: 

**Model #01**
```
[CH-C][Branch-O-[Pb+(10)](#70)#60]-[N+:11]>[Sr(+16)]
>[Ti]
>[Si-Hydrogen-Silicon-Tetrafluoride-(14)-Tetrachloro-Bis(trifluoroacetylmethane)-(18)+>
>>[Pd]+>Pt=[Pt].
`

================================

Based on this dataset we could create three predictive models: 

**Model #01**
```
If (SMiles contains 'CON') then return True else False end if If SMILES does NOT contain '[CH-C][NH+](P)=OS.[OH]=[N+:16]>[Pb]. Then Return TRUE Else false Endif 
End Model 




================================

Based on this dataset we could derive three main patterns: 

Pattern #A - If there exists at least one instance where both C/C(/FC)=CF\c12n[nNH]\cc13ccc(Cl)c(n23)/-c21cccc(c24-nnnnc42)\o11 (labelled as'side effects'), then expect negative value for SMILE's molecular weight; 
    SMILES = [CH]/[F]=\C(\/F)|[c][f]>[\Si].>=[Mg]+.[Cr-Hydrate]-.O>[OH16-[Pt]](#18).[Ti-O-Cu(O)]>>[Metal ion complexes]



================================

Based on this information provided by chemists at our lab (see below), we have identified three possible predictive models: 

**Model #A**
```
{ "input": ["n", "[CH+]=[N+:12][/C]/=[\*N*]\/[Zr].[K].[Ti]>Pd>[Pt], [Ti]}', 'output': ['[n]:18']}
`

================================

Based on this dataset we could create three predictive models: 

**Model #01**
Predicted class (label): { 'yes' }
Features contributing most significantly in predicting these classes:

* CC(c12nccc13[nh-cs-n23)c(F)](Cl)* 
 * SMILES name = C[c%21]=NN11CS/c/[CH%/10]/OS(nccl)%20;molecular weight [M-H]+(+)-[Mg]>SMARTSLIGLQVYDLLGWLRGLRRLALAELLEEQLRELSRRERARAAEAEEAEAAAEGGGSGGAAGGRRAAPPPPAATTTTPPTTAASPSSTSSSPSAASSASTTSRSRTTRSRPRRPPLPVPGGPVPVGAVRVVRVSADTVSVSDGDGTDTDDDGDSDEEDGEESGSSEETTEEEEEMMEKEKKKGSKAKKAQQQRKLTKIITIDENIVTLVDIEEVVEIKDLILKNINNVISFLDKLGIPFKDFVKVLNEEFIRSNIIPIPKAYFRPNLPFFSYFTLYNYLNLFYLNLNRIFSFYNLIHKYYFNFSFIYPNFVFMYNIHFYSFDVIYGTYFGSIHNIMRYGIHYGYAFGFMLPYNTLMRFHIHSIFYGNMIHTGMRIHRGVHHMFNGMMMSMTTNMGFMAMTMNMMDMNMRMHMKHMTHRMVMMAANMQDMRGWMNAFAAIHLPMIAAHMPVAACSHLVPHLCCLCGICCVVCRCCECMCRCAHCCKCDCTTCDCMCFCPCGCQCWCWRCPDWCFECWDWSWNPDWWAWPFWEWFPEWLFWFPWTWPWHHPHWHEHDHAEPDHDPDAEHGHLDLAALLLAGLTVALVTILLFIGLASDRFEKTTFKSNDYEQTALKSWTDNPDIITSRDIFTLEDIALRNELLAREALEEQQALESSELARKVELLENLERASELTEEWRELERRRWREEEREERSRESSEESEDSETTEDDEDVEDADEDERESSSESSEQSERESTTESERTTEEGERESHREQREAERAEEKERYEARARRVERARAERVAAAAATAATEETAAGEAVEAKEAVAASKADAARDARGREGREDGETTERAIRRALPERARSREFGRAIERARYRAYVARHERARPAGERARGEADERGREATERGEDNERPEDRETREALVRTPEGLEGARMWERARIARNPARPGAAPHGPLRAPTAGPADAPPAPSAPIASFPLEASHALSALTTRLSSLSQLSDLTIADSAILDELLETALLEANKILEENDIELDNVENIEDIANILDINEINDIIDENEIDEINTINEDITEINKIDLNETVIDENTILEDIENTIDDINGDIVEDIANDIGNISEIDSINSIESDISGINSENIDIENSINESDNSISSDESISTDNAISHSIDNSSANSISEDNSTAINSDKISMENSESTDITESDEFSTESENTITTENVETSDEVTELANTAITANNIASANEVISANAITYANGMITANYASYATSAYSISAATTNASACTGANICTGISCIENGICYCYISCICSICKCSSCSIACKCSVASCESCICECESCIDCATICAICHACECLKCEEIIIACLCELSCRICESCLECALCTLCAAACHCLAECTICALCAMCHAICLECREMLEAMLAMELEMELYMERAMAEMAERMERNMARAIMRAMREAMARAMMEMRNAIMEASMARENAMESMANRENAMSIREMATDMAIRMADMAMDADIADDIDAEIFDDLDIRLANMINMETGENGMTGTKDATDTDTRAAGESAFEETFSSFSTRSIGTWMSGWAFAQMWXFXDXIXXXXXXXXXXXXXXXXXXXYZ

================================

I would conclude by saying The three molecular descriptors (molecular weight <400; number C atoms >10 & S atom present in SMILES).

================================

Based on this dataset we could create three predictive models: 

**Model #01**
```
If SMiles contains 'P' then return True else False 
OR If [CH-C]=[Pb].InChloride or C.[Si]>Mn>[Ti]
Return model prediction as Yes (label = "Yes") if both conditions met otherwise No(label="No")
Predictive accuracy : {TP - FP / TP + FN}
Precision    {TN-Fp}  /{Totale number samples in test set }
Recall       FP        /Totle no.of Positve Samples predicted by Model from Test Set
 
Accuracy     100%      /
Specificity   99%
sensitivity         95%

AUC value          98 %
Confusion matrix   
True Positive    
False Negative     
false positive      
true negative        
fpr               rec  
tpt              tnr  

ROCs curve plot   

Receiver operating characteristic(Roc)
Area under Curve(Auc):           96 %


Performance metrics for class balance correction methods :
Balanced Accuracy            97%, Balancing Method Used None 


Classification report         
precision recall fbeta-score support          
yes             93%/92%.       
no                89%-90%.

classifier confusion matricu            
tp tp fp fn tn tnr tfpn nfnp             
y y n x xn xp xy nx px pxy fx fy pfyp npxn 
 
confusioon matriux values perforamce metric calculation code here will be added later after testing performance results using different classification algorithms like SVM, Decision Tree etc..
perfromance evaluation result summary table below shows how well our trained classifier performs against various machine learning algorithm used during training process.


Algorithm Name | Precision| Recal |
---------------|-:---------|
DecisionTreeClassifier()|(91%)||85%(|

RandomForestRegressor()(86%),82%)

KNeighborsRegresioner(),79),75())

GradientBoostingRegressione,(78)%73()

ElasticNet Regressior),(77,%71())



The best performing Algorithm was Gradient Boosted Regression which achieved precision score around ~88%% but still lower than expected so need further tuning before deploying into production environment.





================================

Based on this dataset we could create three predictive models: 

Model #01 (Accuracy = ~75%): 
If there exists C/C(/NS)=P(Cl)/SN=[CH]/NH.C/N; then SMILES will be labeled as 'activity' otherwise being inactive for infection/infection-related diseases

================================

Based on this dataset we could create three predictive models: 

**Model A (Accuracy = ~75%)**
If there exists C in SMILES String then predicted class will be 'label' as well if both CCNN & CNCS present at same time; otherwise negative value for F or Cl would increase chances by about +10% more likely than positive values like Br etc., but only when all these conditions hold true together.


```
In [9]:
from sklearn import preprocessing


def get_model():
    le_learner_label_encoders()
    
classifier_a =
preprocessors.LabelEncoder().fit_transform(['smiles_string'])
y_train_classification=[['negative','toxic'], ['positive','not toxic']]
Xtrain_data,ytrainingdata,
xtest_datalabels=y_test_labels
    
clf=a.svm.SVC(kernel='linear')
model_fitted_claassifyer(clfs)
print("Classifying model trained successfully")
predicted_classes=predictor.predict(xteach_labeledata)

accuracy_score(ytrue,prediction,[accuarciescore])
if accuracyscore >.65 :
   return clf,a,b,c,d,e,f,g,h,i,j,k,l,m,n,o.p,q,r,s,t,u,v,w,x.y,z,A,B,C,D,E,F,G,H,I,J,K,L,M,N,Q,R,S,T,U,V,W,X,Y,Z 
else : return None 


get_models()

Out[]:


[[('Negative')]]
{'neg': {'bond_type':'conjugated'}, # Conjungation bonds play important role here so bong type should have conjugate bonding pattern else its considered non-toic



[['Positive']]

`{}`:`{}`

{{}}:{}



================================

Based on this information provided by our chemists at LabX.com (Labx), here's what we recommend: 

Rule #01 - If there exists both amide bonds (-CONR'-C = O where R &'could be hydrogen or alkyl group(s)) as well amino groups in one compound then its likely going have some sort anti-infection property; otherwise if only either type present but no other than these two types will probably lack such ability.




================================

Based on this information provided by our chemists at LabX.com we have identified three main structural features in molecules which could be used as predictive indicators for their ability or lack thereof (labelled'side effects' vs no known negative impact respectively). These include: 

**Feature #A - Presence Of Sulfonic Acid Group**
Molecules containing sulfonate groups (-OSOO^-), such those listed under labels "Stephenson's acid", "[^18][FDG]", etc., appear more likely than others without these functionalities ("No Known Negative Impact")to exhibit undesirable reactions within biological systems; thus they would receive higher scores when predicting potential toxicity based upon molecular structure alone.


```python
def feature_a(mol):
    return mol.surface_area_sulfo_group > threshold_feature_A

feature_list.append(feature_label + str(len(features)+len(rule_features)))
features.update({str(i): []for i 
                in range(threshold Feature A)});
ruleFeatures.extend([threshold_Feature_B,
                     len(list(filter(lambda x:
                                   x.count('s') == count_S_letters),
                                mol.get_substring_from_smiles(
                                    "SMILES")))]); 

print("\nFound %d possible rule structures using SMART filter"%count_RuleStructures)

if show_rule_structues:

  sortedRuleStructure = sorted(countingResults.items(), key=lambda item :item [moleculeCount]

   nRulesToShow=min(nRULE_TO_SHOW,max(RUleLength))
    
     i=nRows*max((int(round(max(rulingLenght)/10)), max(int(item[i]), int(items[nItems-i]))))+rowOffset;
  
      while nRow < rowLimit :
         print("%6c%20r%-30b %-40e%
               %s"
              %nItem,i,rating,

             "%15g%"+"%.9E"%(score_molecular_weight*mw_factor)*weightFactor,molecularWeight(weight=molar_masses[m],mw_type='MW'),


           %",16a"-25%,percentage_of_hydrogens_in_all_bonded_to_heavy_atoms,"%d/%dm (%05.f)"%(percentHydropathy,hypohydophilicityPercentage),'','');

          #print(f"{colorize('%06{self.color}'}')} {rating:<24}{end}", end="")
   
       else:nrows+=min(row_limit,nrules_toshow-nitems)


         ## Add Rule Structue ##



elif plot_results==True:


import matplotlib.pyplot


plt.figure(figsize=(8,7))


ax.set_xlim(left=-100000,
right=len(smile_string)-50000)


fig.show()






================================

Based on this dataset we could create three predictive models using various machine learning algorithms such as SVM (Support Vector Machine), Decision Tree or Naive Bayes Classifier etc., but here I will be presenting one model based upon Random Forest Algorithm which gives good accuracy results in predicting if given SMILES strings would have any effects associated by infection &/or Infant mortality rate: 

```
from sklearn import ensemble
 
# Load our training set from file 'training_set.csv'
data = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/smiles-infection-inf.pdf')
 
X_train  =pandas.DataFrame(data.iloc[:, :-10].values)
ytrain   =np.array(panda.dataframe.loc[data.columns[-9], :])
Ytest    =ytrian.reshape(-one, )
pandadf=datailoc[:,: -11]
x_test_=x_tran.values.pndasdfloc[pnadascolumns[:-13]]
rfc_model.fit(Xt,y_t)

print("Accuracy Score:")
accuracy_score(yte,Yst,rnfclabels=[label for index,lablein enumerate(train_labels)],average=None,n_classes=len(set(list(label)))))
    
def random_forests(x):
return rfc_classify.predict(rnc_claifit,x).ravel()
random forests(smile_string)=predict(random_foests(),smilstring))
result[randomforest(result)]
  
for i,j,k,p,q,t,u,v,w,z,a,b,c,d,e,f,g,h,i,o,s,m:n=labeledata.index[i]:
         print(f"{i} {j}{k},{q},\n{e}")
        
import pandas
    
pd.set_option(display.max_rows None)#display all rows without truncation
        
labels=['Label','side_effect_activity']
features=["SMILE"]
dataset(pd.concat((list(features)),axis='rows'), labels))

model.score(training_data['feature'], trainig_label,
            testing_dtaat ['feautre'],
             expected_outcome_list(testing_labeL),
              multiclass=False)


classification_report(modelscore(
                confusion_matrix(classificationscore(),
                                actualoutcomes()),
               precision_recall_curve(clssicationreport()))
                    metrics.classIFICATION报告())
       
plot_confussion_matrix(confusioonmatrix(report()),title="Confuion Matrix")
plt.show()

roc_auc_plot.confusions matrix(report()))

fig(figsize=(8,-16)),
ax=sns.hue plot(scores["Actual Outcom"], scores[
"Predicted outcome"],
                  hues=True,colorshade=true,true_color=false，legend_ncolor）。
  
axes.sca（“top”、 loc＝‘upper right’！： ax_histogram；histogram_pocie_sensitiivity_and specificity(ax_xact_pracision_rorcurves()．practical정보曲线(highlight_threshold=
True,
highlight_threshold_value =
None ))
      
     
     




================================

